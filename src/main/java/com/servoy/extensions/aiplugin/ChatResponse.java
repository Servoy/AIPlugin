package com.servoy.extensions.aiplugin;

import org.mozilla.javascript.annotations.JSFunction;

import com.servoy.j2db.documentation.ServoyDocumented;

import dev.langchain4j.model.output.FinishReason;
import dev.langchain4j.model.output.TokenUsage;

@ServoyDocumented(scriptingName = "ChatClient")
public class ChatResponse {

	// Wrapper around the langchain4j ChatResponse and a pre-formatted full response string.
	private final dev.langchain4j.model.chat.response.ChatResponse chatResponse;
	private final String fullResponse;
	private final String userMessage;

	/**
	 * Creates a new ChatResponse wrapper.
	 * @param userMessage 
	 *
	 * @param chatResponse the original langchain4j ChatResponse instance returned by the model; must not be null
	 * @param fullResponse the full textual response as presented to the user (may be the same as the AI message content
	 *                     or may include aggregated/processed content); must not be null
	 */
	public ChatResponse(String userMessage, dev.langchain4j.model.chat.response.ChatResponse chatResponse, String fullResponse) {
		this.userMessage = userMessage;
		this.chatResponse = chatResponse;
		this.fullResponse = fullResponse;
	}
	
	/**
	 * Returns the unique identifier for this chat response.
	 * This is typically generated by the underlying model or client library and can be
	 * used to correlate logs, trace requests, or debug conversation history.
	 *
	 * @return the response id as a String, or null if the underlying response does not provide one
	 */
	@JSFunction
	public String getId() {
		return chatResponse.id();
	}

	/**
	 * Returns the 'thinking' text produced by the AI message, if any.
	 * Some models or streaming APIs include incremental "thinking" or intermediate text
	 * while composing the final answer. This method exposes that value from the
	 * underlying langchain4j AI message.
	 *
	 * @return the thinking text, or null if none is available
	 */
	@JSFunction
	public String getThinking() {
		return chatResponse.aiMessage().thinking();
	}
	/**
	 * Returns the original user prompt that led to this response.
	 *
	 * @return the user prompt
	 */
	@JSFunction
	public String getPrompt() {
		return userMessage;
	}

	/**
	 * Returns the full response text that should be shown to the end user.
	 * This value was provided when constructing this wrapper and may include
	 * post-processed, combined or trimmed text in addition to the raw AI message.
	 *
	 * @return the full response text (never null if constructed correctly)
	 */
	@JSFunction
	public String getResponse() {
		return fullResponse;
	}
	
	/**
	 * Returns the reason why the model finished generating the response.
	 * For example, the finish reason could indicate it completed normally or was stopped
	 * because of reaching a token limit. See {@link FinishReason} for possible values.
	 *
	 * @return the finish reason reported by the underlying response, or null if not provided
	 */
	@JSFunction
	public FinishReason getFinishReason() {
		return chatResponse.finishReason();
	}
	
	/**
	 * Returns token usage information for this response.
	 * Token usage typically contains counts for prompt, completion and total tokens
	 * and can be useful for billing, monitoring or debugging token consumption.
	 *
	 * @return the token usage details reported by the underlying response, or null if not available
	 */
	@JSFunction
	public TokenUsage getTokenUsage() {
		return chatResponse.tokenUsage();
	}

	
}